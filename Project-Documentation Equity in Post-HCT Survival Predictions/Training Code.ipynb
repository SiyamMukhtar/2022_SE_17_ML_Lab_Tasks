{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfceea13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\softwares\\Anaconda\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "C:\\softwares\\Anaconda\\lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "C:\\softwares\\Anaconda\\lib\\site-packages\\dask\\dataframe\\__init__.py:42: FutureWarning: \n",
      "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
      "\n",
      "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
      "This will raise in a future version.\n",
      "\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14368\\8584022.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mlifelines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconcordance_index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "from lightgbm import log_evaluation\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from lifelines import NelsonAalenFitter\n",
    "from lifelines.utils import concordance_index\n",
    "import joblib\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "\n",
    "# Load Training Data\n",
    "train = pd.read_csv(\"/kaggle/input/cibmtr-data/train.csv\")\n",
    "\n",
    "# Identify numerical and categorical columns\n",
    "num_cols = train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "cat_cols = train.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "num_cols.remove('efs')\n",
    "num_cols.remove('efs_time')\n",
    "\n",
    "# Handle missing values\n",
    "num_imputer = KNNImputer(n_neighbors=7)\n",
    "train[num_cols] = num_imputer.fit_transform(train[num_cols])\n",
    "\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "train[cat_cols] = cat_imputer.fit_transform(train[cat_cols])\n",
    "\n",
    "# Function to detect and cap outliers automatically using IQR\n",
    "def cap_outliers_auto(df, columns):\n",
    "    for col in columns:\n",
    "        Q1 = df[col].quantile(0.25)  # First quartile (25th percentile)\n",
    "        Q3 = df[col].quantile(0.75)  # Third quartile (75th percentile)\n",
    "        IQR = Q3 - Q1  # Interquartile range\n",
    "        lower_bound = Q1 - 1.5 * IQR  # Lower bound\n",
    "        upper_bound = Q3 + 1.5 * IQR  # Upper bound\n",
    "        \n",
    "        # Cap outliers\n",
    "        df[col] = np.where(df[col] < lower_bound, lower_bound, df[col])\n",
    "        df[col] = np.where(df[col] > upper_bound, upper_bound, df[col])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply outlier handling\n",
    "train = cap_outliers_auto(train, num_cols)\n",
    "\n",
    "print(\"Outliers successfully capped using dynamic IQR calculation!\")\n",
    "\n",
    "# Feature Selection\n",
    "RMV = [\"ID\", \"efs\", \"efs_time\", \"y\"]\n",
    "FEATURES = [c for c in train.columns if c not in RMV]\n",
    "\n",
    "# Encode Categorical Features\n",
    "for col in train.select_dtypes(include=['object', 'category']).columns:\n",
    "    train[col] = train[col].astype('category').cat.codes\n",
    "\n",
    "# Nelson-Aalen Target Transformation\n",
    "def create_nelson(data):\n",
    "    naf = NelsonAalenFitter(nelson_aalen_smoothing=0)\n",
    "    naf.fit(durations=data['efs_time'], event_observed=data['efs'])\n",
    "    return naf.cumulative_hazard_at_times(data['efs_time']).values * -1\n",
    "\n",
    "train[\"y_nel\"] = create_nelson(train)\n",
    "train.loc[train.efs == 0, \"y_nel\"] = (-(-train.loc[train.efs == 0, \"y_nel\"])**0.5)\n",
    "\n",
    "def logit_transform(y, eps=2e-2, eps_mul=1.1):\n",
    "    y = (y - y.min() + eps) / (y.max() - y.min() + eps_mul * eps)\n",
    "    return np.log(y / (1 - y))\n",
    "\n",
    "train[\"y_transformed\"] = logit_transform(train[\"y_nel\"])\n",
    "\n",
    "# Stratified KFold for Cross-Validation\n",
    "FOLDS = 20\n",
    "skf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=42)\n",
    "train[\"fold\"] = -1\n",
    "for fold, (_, val_idx) in enumerate(skf.split(train, train[\"race_group\"])):\n",
    "    train.loc[val_idx, \"fold\"] = fold\n",
    "\n",
    "# Model Training and Stacking\n",
    "xgb_oof, lgb_oof, cat_oof = np.zeros(len(train)), np.zeros(len(train)), np.zeros(len(train))\n",
    "xgb_models, lgb_models, cat_models = [], [], []\n",
    "\n",
    "for fold in range(FOLDS):\n",
    "    x_train, y_train = train.loc[train.fold != fold, FEATURES], train.loc[train.fold != fold, \"y_transformed\"]\n",
    "    x_valid, y_valid = train.loc[train.fold == fold, FEATURES], train.loc[train.fold == fold, \"y_transformed\"]\n",
    "    \n",
    "    # XGBoost Model\n",
    "    model_xgb = xgb.XGBRegressor(n_estimators=1000, learning_rate=0.01, max_depth=4, subsample=0.8)\n",
    "    model_xgb.fit(x_train, y_train, eval_set=[(x_valid, y_valid)], verbose=True)\n",
    "    xgb_models.append(model_xgb)\n",
    "    xgb_oof[train.index[train.fold == fold]] = model_xgb.predict(x_valid)\n",
    "    \n",
    "    # LightGBM Model\n",
    "    model_lgb = lgb.LGBMRegressor(n_estimators=1000, learning_rate=0.01, max_depth=6, num_leaves=31)\n",
    "    model_lgb.fit(x_train, y_train, eval_set=[(x_valid, y_valid)], callbacks=[log_evaluation(500)])\n",
    "    lgb_models.append(model_lgb)\n",
    "    lgb_oof[train.index[train.fold == fold]] = model_lgb.predict(x_valid)\n",
    "    \n",
    "    # CatBoost Model\n",
    "    model_cat = cb.CatBoostRegressor(iterations=1000, learning_rate=0.01, depth=6, verbose=500)\n",
    "    model_cat.fit(x_train, y_train, eval_set=[(x_valid, y_valid)], verbose=True)\n",
    "    cat_models.append(model_cat)\n",
    "    cat_oof[train.index[train.fold == fold]] = model_cat.predict(x_valid)\n",
    "\n",
    "# Save Models\n",
    "joblib.dump(xgb_models, \"xgboost_models.pkl\")\n",
    "joblib.dump(lgb_models, \"lightgbm_models.pkl\")\n",
    "joblib.dump(cat_models, \"catboost_models.pkl\")\n",
    "print(\"Models saved successfully.\")\n",
    "\n",
    "# Prepare Training Data for Meta-Model\n",
    "stacked_train = np.vstack((xgb_oof, lgb_oof, cat_oof)).T\n",
    "\n",
    "# Define Neural Network Meta-Model (Improved)\n",
    "meta_model = keras.Sequential([\n",
    "    layers.Dense(256, kernel_initializer='he_normal', input_shape=(3,)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.LeakyReLU(),\n",
    "    layers.Dropout(0.3),\n",
    "\n",
    "    layers.Dense(128, kernel_initializer='he_normal'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.LeakyReLU(),\n",
    "    layers.Dropout(0.3),\n",
    "\n",
    "    layers.Dense(64, kernel_initializer='he_normal'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.LeakyReLU(),\n",
    "    layers.Dropout(0.2),\n",
    "\n",
    "    layers.Dense(1, activation='linear')  # Output layer\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "meta_model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "                   loss='mse')\n",
    "\n",
    "# Train meta-model\n",
    "meta_model.fit(stacked_train, train[\"y_transformed\"], epochs=30, batch_size=16, verbose=1)\n",
    "\n",
    "# Save Meta-Model\n",
    "meta_model.save(\"meta_model.h5\")\n",
    "print(\"Meta-model saved successfully.\")\n",
    "\n",
    "# Save preprocessors for inference\n",
    "joblib.dump(num_imputer, \"num_imputer.pkl\")\n",
    "joblib.dump(cat_imputer, \"cat_imputer.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
